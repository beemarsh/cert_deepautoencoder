{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1caeac2-f440-4b77-91d6-d6f869de936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 19:20:53.868448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730683254.030162   26340 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730683254.060924   26340 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-03 19:20:54.361598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "import os,datetime\n",
    "\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98f7d8-f9ad-4a10-acfb-c0a10ea6d69c",
   "metadata": {},
   "source": [
    "## Take HTTP data as input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be9ecc3-3d1c-405a-ba0d-cb7ba132bed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{V1Y4-S2IR20QU-6154HFXJ}</td>\n",
       "      <td>2010-01-02 06:55:16</td>\n",
       "      <td>LRR0148</td>\n",
       "      <td>PC-4275</td>\n",
       "      <td>http://msn.com/The_Human_Centipede_First_Seque...</td>\n",
       "      <td>remain representatives consensus concert altho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{Q5R1-T3EF87UE-2395RWZS}</td>\n",
       "      <td>2010-01-02 07:00:13</td>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{X9O1-O0XW52VO-5806RPHG}</td>\n",
       "      <td>2010-01-02 07:03:46</td>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{G5S8-U5OG04TE-5299CCTU}</td>\n",
       "      <td>2010-01-02 07:05:26</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{L0R4-A9DH29VP-4553AUWM}</td>\n",
       "      <td>2010-01-02 07:05:52</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>{Q6Z1-C5EQ59RR-6055BPNL}</td>\n",
       "      <td>2010-01-02 15:08:29</td>\n",
       "      <td>RZC0746</td>\n",
       "      <td>PC-7500</td>\n",
       "      <td>http://nfl.com/Greece_runestones/dybeck/snzvyl...</td>\n",
       "      <td>needed 371 authority elevated daily 8th becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>{M3N3-N2HQ40XO-3795GDSL}</td>\n",
       "      <td>2010-01-02 15:08:34</td>\n",
       "      <td>NAH0503</td>\n",
       "      <td>PC-8267</td>\n",
       "      <td>http://vistaprint.com/Rosendale_trestle/rosend...</td>\n",
       "      <td>alert dangerous lifeguards affected inhg heavy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>{D6N7-T8IV48CS-2221HLNT}</td>\n",
       "      <td>2010-01-02 15:08:35</td>\n",
       "      <td>NWK0215</td>\n",
       "      <td>PC-8370</td>\n",
       "      <td>http://nfl.com/Greece_runestones/dybeck/snzvyl...</td>\n",
       "      <td>lauded generational 24 defended well reclaimed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>{P1A4-K6ZK72FU-1547SSKT}</td>\n",
       "      <td>2010-01-02 15:08:44</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://shareasale.com/1962_South_Vietnamese_In...</td>\n",
       "      <td>military accept museum when aground making rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>{P2N8-C1SA14GT-0797APOO}</td>\n",
       "      <td>2010-01-02 15:08:51</td>\n",
       "      <td>FOB0756</td>\n",
       "      <td>PC-6204</td>\n",
       "      <td>http://seobook.com/Film_Booking_Offices_of_Ame...</td>\n",
       "      <td>3 maximum losses matured eye april rip eve inc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id                date     user       pc  \\\n",
       "0     {V1Y4-S2IR20QU-6154HFXJ} 2010-01-02 06:55:16  LRR0148  PC-4275   \n",
       "1     {Q5R1-T3EF87UE-2395RWZS} 2010-01-02 07:00:13  NGF0157  PC-6056   \n",
       "2     {X9O1-O0XW52VO-5806RPHG} 2010-01-02 07:03:46  NGF0157  PC-6056   \n",
       "3     {G5S8-U5OG04TE-5299CCTU} 2010-01-02 07:05:26  IRM0931  PC-7188   \n",
       "4     {L0R4-A9DH29VP-4553AUWM} 2010-01-02 07:05:52  IRM0931  PC-7188   \n",
       "...                        ...                 ...      ...      ...   \n",
       "2995  {Q6Z1-C5EQ59RR-6055BPNL} 2010-01-02 15:08:29  RZC0746  PC-7500   \n",
       "2996  {M3N3-N2HQ40XO-3795GDSL} 2010-01-02 15:08:34  NAH0503  PC-8267   \n",
       "2997  {D6N7-T8IV48CS-2221HLNT} 2010-01-02 15:08:35  NWK0215  PC-8370   \n",
       "2998  {P1A4-K6ZK72FU-1547SSKT} 2010-01-02 15:08:44  IRM0931  PC-7188   \n",
       "2999  {P2N8-C1SA14GT-0797APOO} 2010-01-02 15:08:51  FOB0756  PC-6204   \n",
       "\n",
       "                                                    url  \\\n",
       "0     http://msn.com/The_Human_Centipede_First_Seque...   \n",
       "1     http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "2     http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "3     http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "4     http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "...                                                 ...   \n",
       "2995  http://nfl.com/Greece_runestones/dybeck/snzvyl...   \n",
       "2996  http://vistaprint.com/Rosendale_trestle/rosend...   \n",
       "2997  http://nfl.com/Greece_runestones/dybeck/snzvyl...   \n",
       "2998  http://shareasale.com/1962_South_Vietnamese_In...   \n",
       "2999  http://seobook.com/Film_Booking_Offices_of_Ame...   \n",
       "\n",
       "                                                content  \n",
       "0     remain representatives consensus concert altho...  \n",
       "1     festival off northwards than congestion partne...  \n",
       "2     long away reorganized baldwin seth business 18...  \n",
       "3     among german schwein experimental becomes prev...  \n",
       "4     kate criteria j 2008 highest 12 include books ...  \n",
       "...                                                 ...  \n",
       "2995  needed 371 authority elevated daily 8th becaus...  \n",
       "2996  alert dangerous lifeguards affected inhg heavy...  \n",
       "2997  lauded generational 24 defended well reclaimed...  \n",
       "2998  military accept museum when aground making rel...  \n",
       "2999  3 maximum losses matured eye april rip eve inc...  \n",
       "\n",
       "[3000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../r4.2/http.csv\",nrows=3000)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaac3b5-6b03-4cfc-8030-40611181a467",
   "metadata": {},
   "source": [
    "## Extract features from Date!\n",
    "The user logs are at different times. I divided the time into **4** different time frames.\n",
    "- 0 = 12AM - 6AM\n",
    "- 1 = 6AM - 12PM\n",
    "- 2 = 12PM - 6PM\n",
    "- 3 = 6PM - 12AM\n",
    "Therefore, a new feature **time_frame** is made. Date is decomposed into 3 other numerical features: `day`,`month`, and `year`. Finally, date feature is dropped.\n",
    "\n",
    "After dividng them into 4 different time frames in order to apply one-hot encoding.\n",
    "\n",
    "**Why One-Hot?**\n",
    "  - **No Ordinality:** Each time frame is represented independently without implying any order.\n",
    "  - **Clarity:** Clearly distinguishes between different time frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4da3be06-8f5b-4546-85fd-efeaea79b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>time_frame_1</th>\n",
       "      <th>time_frame_2</th>\n",
       "      <th>time_frame_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LRR0148</td>\n",
       "      <td>PC-4275</td>\n",
       "      <td>http://msn.com/The_Human_Centipede_First_Seque...</td>\n",
       "      <td>remain representatives consensus concert altho...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>JIM0095</td>\n",
       "      <td>PC-9328</td>\n",
       "      <td>http://city-data.com/No_Way_Out_2004/hotty/sre...</td>\n",
       "      <td>enlarged under generic advantage vision do any...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>BBS0039</td>\n",
       "      <td>PC-9436</td>\n",
       "      <td>http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...</td>\n",
       "      <td>an unknown afternoon dietary state law nationa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>KAL0395</td>\n",
       "      <td>PC-0004</td>\n",
       "      <td>http://tigerdirect.com/European_Commission/bar...</td>\n",
       "      <td>begin top we band themselves harshly or fourth...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>LDB0090</td>\n",
       "      <td>PC-6824</td>\n",
       "      <td>http://microsoft.com/Meteorological_history_of...</td>\n",
       "      <td>until acquire flared get secondary minas sea u...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>WTF0387</td>\n",
       "      <td>PC-6159</td>\n",
       "      <td>http://sidereel.com/Miniopterus_griveaudi/griv...</td>\n",
       "      <td>form air rely conference quickly set expected ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user       pc                                                url  \\\n",
       "0      LRR0148  PC-4275  http://msn.com/The_Human_Centipede_First_Seque...   \n",
       "1      NGF0157  PC-6056  http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "2      NGF0157  PC-6056  http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "3      IRM0931  PC-7188  http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "4      IRM0931  PC-7188  http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "...        ...      ...                                                ...   \n",
       "99995  JIM0095  PC-9328  http://city-data.com/No_Way_Out_2004/hotty/sre...   \n",
       "99996  BBS0039  PC-9436  http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...   \n",
       "99997  KAL0395  PC-0004  http://tigerdirect.com/European_Commission/bar...   \n",
       "99998  LDB0090  PC-6824  http://microsoft.com/Meteorological_history_of...   \n",
       "99999  WTF0387  PC-6159  http://sidereel.com/Miniopterus_griveaudi/griv...   \n",
       "\n",
       "                                                 content  day  month  year  \\\n",
       "0      remain representatives consensus concert altho...    2      1  2010   \n",
       "1      festival off northwards than congestion partne...    2      1  2010   \n",
       "2      long away reorganized baldwin seth business 18...    2      1  2010   \n",
       "3      among german schwein experimental becomes prev...    2      1  2010   \n",
       "4      kate criteria j 2008 highest 12 include books ...    2      1  2010   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "99995  enlarged under generic advantage vision do any...    5      1  2010   \n",
       "99996  an unknown afternoon dietary state law nationa...    5      1  2010   \n",
       "99997  begin top we band themselves harshly or fourth...    5      1  2010   \n",
       "99998  until acquire flared get secondary minas sea u...    5      1  2010   \n",
       "99999  form air rely conference quickly set expected ...    5      1  2010   \n",
       "\n",
       "       time_frame_1  time_frame_2  time_frame_3  \n",
       "0                 1             0             0  \n",
       "1                 1             0             0  \n",
       "2                 1             0             0  \n",
       "3                 1             0             0  \n",
       "4                 1             0             0  \n",
       "...             ...           ...           ...  \n",
       "99995             1             0             0  \n",
       "99996             1             0             0  \n",
       "99997             1             0             0  \n",
       "99998             1             0             0  \n",
       "99999             1             0             0  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function to categorize time frames\n",
    "def categorize_time_frame(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 0\n",
    "    elif 6 <= hour < 12:\n",
    "        return 1\n",
    "    elif 12 <= hour < 18:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Apply the function to create the 'time_frame' column\n",
    "# new_df = df.copy()\n",
    "df['time_frame'] = df['date'].dt.hour.apply(categorize_time_frame)\n",
    "df['day'] = df['date'].dt.day\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "df=df.drop(columns=\"date\")\n",
    "df=df.drop(columns=\"id\")\n",
    "\n",
    "# Applying one-hot encoding\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "time_encoded = ohe.fit_transform(df[['time_frame']])\n",
    "\n",
    "# Create DataFrame with One-Hot Encoded Columns\n",
    "time_encoded_df = pd.DataFrame(time_encoded.astype(int), columns=ohe.get_feature_names_out(['time_frame']))\n",
    "\n",
    "# Concatenate with Original DataFrame\n",
    "df = pd.concat([df, time_encoded_df], axis=1).drop('time_frame', axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a39d560-1ed3-4bca-831e-140e8c1056e5",
   "metadata": {},
   "source": [
    "# Label Encoding Year to reduce its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ff1357d-3cd2-471a-9f80-af1ddbd6d1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>time_frame_1</th>\n",
       "      <th>time_frame_2</th>\n",
       "      <th>time_frame_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LRR0148</td>\n",
       "      <td>PC-4275</td>\n",
       "      <td>http://msn.com/The_Human_Centipede_First_Seque...</td>\n",
       "      <td>remain representatives consensus concert altho...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>JIM0095</td>\n",
       "      <td>PC-9328</td>\n",
       "      <td>http://city-data.com/No_Way_Out_2004/hotty/sre...</td>\n",
       "      <td>enlarged under generic advantage vision do any...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>BBS0039</td>\n",
       "      <td>PC-9436</td>\n",
       "      <td>http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...</td>\n",
       "      <td>an unknown afternoon dietary state law nationa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>KAL0395</td>\n",
       "      <td>PC-0004</td>\n",
       "      <td>http://tigerdirect.com/European_Commission/bar...</td>\n",
       "      <td>begin top we band themselves harshly or fourth...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>LDB0090</td>\n",
       "      <td>PC-6824</td>\n",
       "      <td>http://microsoft.com/Meteorological_history_of...</td>\n",
       "      <td>until acquire flared get secondary minas sea u...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>WTF0387</td>\n",
       "      <td>PC-6159</td>\n",
       "      <td>http://sidereel.com/Miniopterus_griveaudi/griv...</td>\n",
       "      <td>form air rely conference quickly set expected ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user       pc                                                url  \\\n",
       "0      LRR0148  PC-4275  http://msn.com/The_Human_Centipede_First_Seque...   \n",
       "1      NGF0157  PC-6056  http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "2      NGF0157  PC-6056  http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "3      IRM0931  PC-7188  http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "4      IRM0931  PC-7188  http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "...        ...      ...                                                ...   \n",
       "99995  JIM0095  PC-9328  http://city-data.com/No_Way_Out_2004/hotty/sre...   \n",
       "99996  BBS0039  PC-9436  http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...   \n",
       "99997  KAL0395  PC-0004  http://tigerdirect.com/European_Commission/bar...   \n",
       "99998  LDB0090  PC-6824  http://microsoft.com/Meteorological_history_of...   \n",
       "99999  WTF0387  PC-6159  http://sidereel.com/Miniopterus_griveaudi/griv...   \n",
       "\n",
       "                                                 content  day  month  year  \\\n",
       "0      remain representatives consensus concert altho...    2      1     0   \n",
       "1      festival off northwards than congestion partne...    2      1     0   \n",
       "2      long away reorganized baldwin seth business 18...    2      1     0   \n",
       "3      among german schwein experimental becomes prev...    2      1     0   \n",
       "4      kate criteria j 2008 highest 12 include books ...    2      1     0   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "99995  enlarged under generic advantage vision do any...    5      1     0   \n",
       "99996  an unknown afternoon dietary state law nationa...    5      1     0   \n",
       "99997  begin top we band themselves harshly or fourth...    5      1     0   \n",
       "99998  until acquire flared get secondary minas sea u...    5      1     0   \n",
       "99999  form air rely conference quickly set expected ...    5      1     0   \n",
       "\n",
       "       time_frame_1  time_frame_2  time_frame_3  \n",
       "0                 1             0             0  \n",
       "1                 1             0             0  \n",
       "2                 1             0             0  \n",
       "3                 1             0             0  \n",
       "4                 1             0             0  \n",
       "...             ...           ...           ...  \n",
       "99995             1             0             0  \n",
       "99996             1             0             0  \n",
       "99997             1             0             0  \n",
       "99998             1             0             0  \n",
       "99999             1             0             0  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_year= LabelEncoder()\n",
    "le_year.fit(df['year'])\n",
    "df['year'] = le_year.transform(df['year'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03743bc9-5cf5-44ce-901c-2e868b019c41",
   "metadata": {},
   "source": [
    "# Convert USER and PC based on binary encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd8d9241-2346-4ae4-8467-5fcb723efa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34348/3161611282.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label_encoded_df['user'] = user_le.transform(label_encoded_df.user)\n"
     ]
    }
   ],
   "source": [
    "user_label_encoder_path = \"../encoded_objects/user_label_encoder.pkl\"\n",
    "user_binary_encoder_path = \"../encoded_objects/user_binary_encoder.pkl\"\n",
    "\n",
    "pc_label_encoder_path = \"../encoded_objects/pc_label_encoder.pkl\"\n",
    "pc_binary_encoder_path = \"../encoded_objects/pc_binary_encoder.pkl\"\n",
    "\n",
    "label_encoded_df = df[['user','pc']]\n",
    "\n",
    "with open(user_label_encoder_path, 'rb') as file:\n",
    "    user_le = pickle.load(file)\n",
    "\n",
    "label_encoded_df['user'] = user_le.transform(label_encoded_df.user)\n",
    "\n",
    "with open(user_binary_encoder_path, 'rb') as file:\n",
    "    user_binary_enc = pickle.load(file)\n",
    "\n",
    "label_encoded_df = user_binary_enc.transform(label_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9cf0a23c-44c5-480b-b13f-d4afa96ac522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>time_frame_1</th>\n",
       "      <th>time_frame_2</th>\n",
       "      <th>time_frame_3</th>\n",
       "      <th>user_0</th>\n",
       "      <th>user_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pc_0</th>\n",
       "      <th>pc_1</th>\n",
       "      <th>pc_2</th>\n",
       "      <th>pc_3</th>\n",
       "      <th>pc_4</th>\n",
       "      <th>pc_5</th>\n",
       "      <th>pc_6</th>\n",
       "      <th>pc_7</th>\n",
       "      <th>pc_8</th>\n",
       "      <th>pc_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://msn.com/The_Human_Centipede_First_Seque...</td>\n",
       "      <td>remain representatives consensus concert altho...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>http://city-data.com/No_Way_Out_2004/hotty/sre...</td>\n",
       "      <td>enlarged under generic advantage vision do any...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...</td>\n",
       "      <td>an unknown afternoon dietary state law nationa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>http://tigerdirect.com/European_Commission/bar...</td>\n",
       "      <td>begin top we band themselves harshly or fourth...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>http://microsoft.com/Meteorological_history_of...</td>\n",
       "      <td>until acquire flared get secondary minas sea u...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>http://sidereel.com/Miniopterus_griveaudi/griv...</td>\n",
       "      <td>form air rely conference quickly set expected ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "0      http://msn.com/The_Human_Centipede_First_Seque...   \n",
       "1      http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "2      http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "3      http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "4      http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "...                                                  ...   \n",
       "99995  http://city-data.com/No_Way_Out_2004/hotty/sre...   \n",
       "99996  http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...   \n",
       "99997  http://tigerdirect.com/European_Commission/bar...   \n",
       "99998  http://microsoft.com/Meteorological_history_of...   \n",
       "99999  http://sidereel.com/Miniopterus_griveaudi/griv...   \n",
       "\n",
       "                                                 content  day  month  year  \\\n",
       "0      remain representatives consensus concert altho...    2      1     0   \n",
       "1      festival off northwards than congestion partne...    2      1     0   \n",
       "2      long away reorganized baldwin seth business 18...    2      1     0   \n",
       "3      among german schwein experimental becomes prev...    2      1     0   \n",
       "4      kate criteria j 2008 highest 12 include books ...    2      1     0   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "99995  enlarged under generic advantage vision do any...    5      1     0   \n",
       "99996  an unknown afternoon dietary state law nationa...    5      1     0   \n",
       "99997  begin top we band themselves harshly or fourth...    5      1     0   \n",
       "99998  until acquire flared get secondary minas sea u...    5      1     0   \n",
       "99999  form air rely conference quickly set expected ...    5      1     0   \n",
       "\n",
       "       time_frame_1  time_frame_2  time_frame_3  user_0  user_1  ...  pc_0  \\\n",
       "0                 1             0             0       0       0  ...     0   \n",
       "1                 1             0             0       0       0  ...     0   \n",
       "2                 1             0             0       0       0  ...     0   \n",
       "3                 1             0             0       0       0  ...     0   \n",
       "4                 1             0             0       0       0  ...     0   \n",
       "...             ...           ...           ...     ...     ...  ...   ...   \n",
       "99995             1             0             0       0       0  ...     0   \n",
       "99996             1             0             0       0       0  ...     0   \n",
       "99997             1             0             0       1       0  ...     1   \n",
       "99998             1             0             0       0       0  ...     0   \n",
       "99999             1             0             0       0       1  ...     0   \n",
       "\n",
       "       pc_1  pc_2  pc_3  pc_4  pc_5  pc_6  pc_7  pc_8  pc_9  \n",
       "0         0     0     0     0     0     0     0     0     1  \n",
       "1         0     0     0     0     0     0     0     1     0  \n",
       "2         0     0     0     0     0     0     0     1     0  \n",
       "3         0     0     0     0     0     0     0     1     1  \n",
       "4         0     0     0     0     0     0     0     1     1  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "99995     0     0     1     1     0     0     1     1     0  \n",
       "99996     0     1     0     1     1     0     1     0     1  \n",
       "99997     0     0     0     0     0     1     0     0     0  \n",
       "99998     0     1     0     1     0     1     1     0     0  \n",
       "99999     1     1     1     1     0     1     1     0     1  \n",
       "\n",
       "[100000 rows x 28 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pc_label_encoder_path, 'rb') as file:\n",
    "    pc_le = pickle.load(file)\n",
    "\n",
    "label_encoded_df['pc'] = pc_le.transform(label_encoded_df.pc)\n",
    "\n",
    "with open(pc_binary_encoder_path, 'rb') as file:\n",
    "    pc_binary_enc = pickle.load(file)\n",
    "\n",
    "label_encoded_df = pc_binary_enc.transform(label_encoded_df)\n",
    "\n",
    "# At the end, combine the two encoded users and PC to the entire dataframe\n",
    "\n",
    "df = pd.concat([df, label_encoded_df],axis=1)\n",
    "df = df.drop(columns=['pc','user'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f5400a1-5aaf-428e-8311-c2ddefd90afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('../models/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61e65a36-c199-4e42-9043-f05e24b73e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk data files (if not already installed)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "55a4f5b8-4fa6-4001-a344-65ad20f18b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d4ece9f-7b7b-4ab8-8355-b1d3f3bfee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Lowercase the text and remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b9ba37fd-0818-4cd5-9fab-107946599fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>time_frame_1</th>\n",
       "      <th>time_frame_2</th>\n",
       "      <th>time_frame_3</th>\n",
       "      <th>user_0</th>\n",
       "      <th>user_1</th>\n",
       "      <th>user_2</th>\n",
       "      <th>...</th>\n",
       "      <th>pc_1</th>\n",
       "      <th>pc_2</th>\n",
       "      <th>pc_3</th>\n",
       "      <th>pc_4</th>\n",
       "      <th>pc_5</th>\n",
       "      <th>pc_6</th>\n",
       "      <th>pc_7</th>\n",
       "      <th>pc_8</th>\n",
       "      <th>pc_9</th>\n",
       "      <th>content_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://msn.com/The_Human_Centipede_First_Seque...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.025419906, 0.030988539, 0.0007039663, 0.048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.018577939, 0.038862318, -0.05105591, 0.0962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.029726226, 0.024776326, -0.036115974, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0021921794, 0.045175172, 0.00957133, 0.1425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.053494874, 0.025620727, 0.0054260255, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>http://city-data.com/No_Way_Out_2004/hotty/sre...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.002129632, 0.02529165, -0.014264081, 0.0252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.037862513, 0.056490157, 0.025104947, 0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>http://tigerdirect.com/European_Commission/bar...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.06907099, -0.014849576, -0.023220062, 0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>http://microsoft.com/Meteorological_history_of...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.04055214, 0.06912422, 0.0029149055, 0.08141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>http://sidereel.com/Miniopterus_griveaudi/griv...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0018520355, 0.0883859, 0.03288555, 0.01346...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  day  month  year  \\\n",
       "0      http://msn.com/The_Human_Centipede_First_Seque...    2      1     0   \n",
       "1      http://urbanspoon.com/Plunketts_Creek_Loyalsoc...    2      1     0   \n",
       "2      http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...    2      1     0   \n",
       "3      http://groupon.com/Leonhard_Euler/leonhard/tne...    2      1     0   \n",
       "4      http://flickr.com/Inauguration_of_Barack_Obama...    2      1     0   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "99995  http://city-data.com/No_Way_Out_2004/hotty/sre...    5      1     0   \n",
       "99996  http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...    5      1     0   \n",
       "99997  http://tigerdirect.com/European_Commission/bar...    5      1     0   \n",
       "99998  http://microsoft.com/Meteorological_history_of...    5      1     0   \n",
       "99999  http://sidereel.com/Miniopterus_griveaudi/griv...    5      1     0   \n",
       "\n",
       "       time_frame_1  time_frame_2  time_frame_3  user_0  user_1  user_2  ...  \\\n",
       "0                 1             0             0       0       0       0  ...   \n",
       "1                 1             0             0       0       0       0  ...   \n",
       "2                 1             0             0       0       0       0  ...   \n",
       "3                 1             0             0       0       0       0  ...   \n",
       "4                 1             0             0       0       0       0  ...   \n",
       "...             ...           ...           ...     ...     ...     ...  ...   \n",
       "99995             1             0             0       0       0       0  ...   \n",
       "99996             1             0             0       0       0       1  ...   \n",
       "99997             1             0             0       1       0       0  ...   \n",
       "99998             1             0             0       0       0       1  ...   \n",
       "99999             1             0             0       0       1       1  ...   \n",
       "\n",
       "       pc_1  pc_2  pc_3  pc_4  pc_5  pc_6  pc_7  pc_8  pc_9  \\\n",
       "0         0     0     0     0     0     0     0     0     1   \n",
       "1         0     0     0     0     0     0     0     1     0   \n",
       "2         0     0     0     0     0     0     0     1     0   \n",
       "3         0     0     0     0     0     0     0     1     1   \n",
       "4         0     0     0     0     0     0     0     1     1   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "99995     0     0     1     1     0     0     1     1     0   \n",
       "99996     0     1     0     1     1     0     1     0     1   \n",
       "99997     0     0     0     0     0     1     0     0     0   \n",
       "99998     0     1     0     1     0     1     1     0     0   \n",
       "99999     1     1     1     1     0     1     1     0     1   \n",
       "\n",
       "                                          content_vector  \n",
       "0      [0.025419906, 0.030988539, 0.0007039663, 0.048...  \n",
       "1      [0.018577939, 0.038862318, -0.05105591, 0.0962...  \n",
       "2      [-0.029726226, 0.024776326, -0.036115974, 0.05...  \n",
       "3      [0.0021921794, 0.045175172, 0.00957133, 0.1425...  \n",
       "4      [-0.053494874, 0.025620727, 0.0054260255, 0.01...  \n",
       "...                                                  ...  \n",
       "99995  [0.002129632, 0.02529165, -0.014264081, 0.0252...  \n",
       "99996  [-0.037862513, 0.056490157, 0.025104947, 0.064...  \n",
       "99997  [0.06907099, -0.014849576, -0.023220062, 0.057...  \n",
       "99998  [0.04055214, 0.06912422, 0.0029149055, 0.08141...  \n",
       "99999  [-0.0018520355, 0.0883859, 0.03288555, 0.01346...  \n",
       "\n",
       "[100000 rows x 28 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define function to get sentence vector\n",
    "def get_sentence_vector(text):\n",
    "    words = preprocess(text).split()  # Preprocess and split the text into words\n",
    "    word_vectors = [word2vec_model[word] for word in words if word in word2vec_model]\n",
    "    \n",
    "    # If there are no valid words, return a zero vector\n",
    "    if not word_vectors:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    # Calculate the mean of word vectors for each sentence\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "df['content_vector'] = df['content'].apply(get_sentence_vector)\n",
    "df=df.drop(columns=['content'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5b693-20be-478a-a374-0358b15e2ba8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079cdfd9-b810-4a87-8865-88467b467f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d2531e-54b0-4123-9236-4605d800e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] = df['url'].str.replace(\"_\",' ')\n",
    "df['url_tokens'] = df['url'].str.extractall(r'(\\w+)')[0].groupby(level=0).agg(' '.join)\n",
    "\n",
    "texts = df['url_tokens'].tolist()  # Convert the column of texts into a list\n",
    "df\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each row in the dataset\n",
    "corpus = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Create a dictionary and corpus from processed texts\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "\n",
    "num_topics = list(range(5)[3:])\n",
    "num_keywords = 15\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "\n",
    "for i in num_topics:\n",
    "    LDA_models[i] = LdaModel(corpus=bow_corpus,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=i,\n",
    "                             update_every=1,\n",
    "                             chunksize=len(bow_corpus),\n",
    "                             passes=20,\n",
    "                             alpha='auto',\n",
    "                             random_state=42)\n",
    "\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=num_keywords,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf96ed-3676-40c0-8832-605934e7a6f3",
   "metadata": {},
   "source": [
    "### Now create a function to derive the Jaccard similarity of two topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8d2259-865f-4c52-84eb-5edcbab3b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c965dd-e5e0-493c-b8e3-1dec561eda34",
   "metadata": {},
   "source": [
    "### gensim has a built in model for topic coherence (this uses the 'c_v' option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34458f78-e965-4eba-a1f2-7e52a5f3451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences = [CoherenceModel(model=LDA_models[i], texts=corpus, dictionary=dictionary, coherence='c_v').get_coherence()\\\n",
    "              for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffde4d-d1f0-4053-98a1-6b1dd1cdb005",
   "metadata": {},
   "source": [
    "### From here derive the ideal number of topics roughly through the difference between the coherence and stability per number of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2fc7f7-a307-4ac6-a42f-b9c169b3cda0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_stabilities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m coh_sta_diffs \u001b[38;5;241m=\u001b[39m [coherences[i] \u001b[38;5;241m-\u001b[39m mean_stabilities[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_keywords)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;66;03m# limit topic numbers to the number of keywords\u001b[39;00m\n\u001b[1;32m      2\u001b[0m coh_sta_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(coh_sta_diffs)\n\u001b[1;32m      3\u001b[0m coh_sta_max_idxs \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(coh_sta_diffs) \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m coh_sta_max]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_stabilities' is not defined"
     ]
    }
   ],
   "source": [
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa151f7-d950-4e1b-95ce-a148dd6fb28f",
   "metadata": {},
   "source": [
    "### Finally graph these metrics across the topic numbers:\n",
    "\n",
    "Your ideal number of topics will maximize coherence and minimize the topic overlap based on Jaccard similarity. In this case it looks like we'd be safe choosing topic numbers around 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128cb04-1786-45d6-946e-5470063f4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
    "ax.set_ylabel('Metric Level', fontsize=20)\n",
    "ax.set_xlabel('Number of Topics', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd793a1d-670a-4521-ad34-bb7f44111cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278c554-5cc0-4142-a774-028063e99e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word2vec = np.array(df['content_vector'].tolist())  # Shape: (num_samples, 300)\n",
    "\n",
    "# Assuming other features are already numeric, extract them as a NumPy array\n",
    "X_other_features = df[['user', 'date']].values  # Adjust to your actual features\n",
    "\n",
    "# Concatenate Word2Vec and other features along the last axis\n",
    "X_combined = np.concatenate([X_word2vec, X_other_features], axis=1)\n",
    "X_combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa43f81-ce89-457e-a1fb-c0f93fb10abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
