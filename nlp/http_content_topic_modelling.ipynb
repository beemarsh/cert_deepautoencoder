{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7139e4-4ffe-4d94-853f-28202ff2d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 11:00:01.153729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730044801.197984  124660 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730044801.210477  124660 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-27 11:00:01.249760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os,datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d828191c-a75c-4988-b983-586f89a98d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{V1Y4-S2IR20QU-6154HFXJ}</td>\n",
       "      <td>01/02/2010 06:55:16</td>\n",
       "      <td>LRR0148</td>\n",
       "      <td>PC-4275</td>\n",
       "      <td>http://msn.com/The_Human_Centipede_First_Seque...</td>\n",
       "      <td>remain representatives consensus concert altho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{Q5R1-T3EF87UE-2395RWZS}</td>\n",
       "      <td>01/02/2010 07:00:13</td>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{X9O1-O0XW52VO-5806RPHG}</td>\n",
       "      <td>01/02/2010 07:03:46</td>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{G5S8-U5OG04TE-5299CCTU}</td>\n",
       "      <td>01/02/2010 07:05:26</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{L0R4-A9DH29VP-4553AUWM}</td>\n",
       "      <td>01/02/2010 07:05:52</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>{F2B4-J5HG21CP-2472KNWD}</td>\n",
       "      <td>01/05/2010 08:24:11</td>\n",
       "      <td>JIM0095</td>\n",
       "      <td>PC-9328</td>\n",
       "      <td>http://city-data.com/No_Way_Out_2004/hotty/sre...</td>\n",
       "      <td>enlarged under generic advantage vision do any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>{P4C0-H4AC44QZ-5898ATQE}</td>\n",
       "      <td>01/05/2010 08:24:12</td>\n",
       "      <td>BBS0039</td>\n",
       "      <td>PC-9436</td>\n",
       "      <td>http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...</td>\n",
       "      <td>an unknown afternoon dietary state law nationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>{U9Y2-H0WW90NF-1184ROHW}</td>\n",
       "      <td>01/05/2010 08:24:12</td>\n",
       "      <td>KAL0395</td>\n",
       "      <td>PC-0004</td>\n",
       "      <td>http://tigerdirect.com/European_Commission/bar...</td>\n",
       "      <td>begin top we band themselves harshly or fourth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>{Y2Y4-T1KW57MX-2017DWRE}</td>\n",
       "      <td>01/05/2010 08:24:12</td>\n",
       "      <td>LDB0090</td>\n",
       "      <td>PC-6824</td>\n",
       "      <td>http://microsoft.com/Meteorological_history_of...</td>\n",
       "      <td>until acquire flared get secondary minas sea u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>{X2D5-E5UH86FZ-2678DVEF}</td>\n",
       "      <td>01/05/2010 08:24:12</td>\n",
       "      <td>WTF0387</td>\n",
       "      <td>PC-6159</td>\n",
       "      <td>http://sidereel.com/Miniopterus_griveaudi/griv...</td>\n",
       "      <td>form air rely conference quickly set expected ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id                 date     user       pc  \\\n",
       "0      {V1Y4-S2IR20QU-6154HFXJ}  01/02/2010 06:55:16  LRR0148  PC-4275   \n",
       "1      {Q5R1-T3EF87UE-2395RWZS}  01/02/2010 07:00:13  NGF0157  PC-6056   \n",
       "2      {X9O1-O0XW52VO-5806RPHG}  01/02/2010 07:03:46  NGF0157  PC-6056   \n",
       "3      {G5S8-U5OG04TE-5299CCTU}  01/02/2010 07:05:26  IRM0931  PC-7188   \n",
       "4      {L0R4-A9DH29VP-4553AUWM}  01/02/2010 07:05:52  IRM0931  PC-7188   \n",
       "...                         ...                  ...      ...      ...   \n",
       "99995  {F2B4-J5HG21CP-2472KNWD}  01/05/2010 08:24:11  JIM0095  PC-9328   \n",
       "99996  {P4C0-H4AC44QZ-5898ATQE}  01/05/2010 08:24:12  BBS0039  PC-9436   \n",
       "99997  {U9Y2-H0WW90NF-1184ROHW}  01/05/2010 08:24:12  KAL0395  PC-0004   \n",
       "99998  {Y2Y4-T1KW57MX-2017DWRE}  01/05/2010 08:24:12  LDB0090  PC-6824   \n",
       "99999  {X2D5-E5UH86FZ-2678DVEF}  01/05/2010 08:24:12  WTF0387  PC-6159   \n",
       "\n",
       "                                                     url  \\\n",
       "0      http://msn.com/The_Human_Centipede_First_Seque...   \n",
       "1      http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "2      http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "3      http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "4      http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "...                                                  ...   \n",
       "99995  http://city-data.com/No_Way_Out_2004/hotty/sre...   \n",
       "99996  http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...   \n",
       "99997  http://tigerdirect.com/European_Commission/bar...   \n",
       "99998  http://microsoft.com/Meteorological_history_of...   \n",
       "99999  http://sidereel.com/Miniopterus_griveaudi/griv...   \n",
       "\n",
       "                                                 content  \n",
       "0      remain representatives consensus concert altho...  \n",
       "1      festival off northwards than congestion partne...  \n",
       "2      long away reorganized baldwin seth business 18...  \n",
       "3      among german schwein experimental becomes prev...  \n",
       "4      kate criteria j 2008 highest 12 include books ...  \n",
       "...                                                  ...  \n",
       "99995  enlarged under generic advantage vision do any...  \n",
       "99996  an unknown afternoon dietary state law nationa...  \n",
       "99997  begin top we band themselves harshly or fourth...  \n",
       "99998  until acquire flared get secondary minas sea u...  \n",
       "99999  form air rely conference quickly set expected ...  \n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../r4.2/http.csv\",nrows=100000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f693b78-ffd8-45e0-af64-479190b719f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /homes/01/bxbhusal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /homes/01/bxbhusal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk data files (if not already installed)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c62bd91-7a6a-4900-9b1f-c23d39486f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc218b8-5916-4a73-9499-61b88454f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to preprocess a single document\n",
    "def preprocess(text):\n",
    "    # Lowercase and remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d302e160-e17f-45c1-bbe5-98091d7ca54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content = [preprocess(doc) for doc in df['content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a1636-848b-4ffd-8536-ccf7fd0ce20e",
   "metadata": {},
   "source": [
    "Step 2: Convert Documents to a Format Usable by LDA\n",
    "\n",
    "LDA models typically work with bag-of-words (BoW) or term-frequency-inverse-document-frequency (TF-IDF) representations. You can use gensim to perform both vectorization and LDA modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa41e6d-9b6c-4ece-bdad-e8730c18d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Dictionary before trimming: 45466\n",
      "Length of the Dictionary after trimming: 28705\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Create a dictionary and corpus for the LDA model\n",
    "dictionary = Dictionary(preprocessed_content)\n",
    "\n",
    "# we could olse trim low- and high-frequency words\n",
    "# Here, we're saying filter out words which occur in fewer than N documents and more than M% of the documents\n",
    "\n",
    "print(\"Length of the Dictionary before trimming:\", len(dictionary))\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "print(\"Length of the Dictionary after trimming:\", len(dictionary))\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_content]\n",
    "\n",
    "\n",
    "# Optional: Convert to TF-IDF format\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f505eb-09be-4028-9b74-46fa64967a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bf237e-84d9-45ca-956f-e57389533a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics\n",
    "num_topics = 10  # This should be tuned based on your data and requirements\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c24ece-671d-4bd2-8aaf-6dda731e24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get topic distribution for a document\n",
    "def get_topic_vector(doc_bow):\n",
    "    topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    # Convert to a fixed-length vector for each document\n",
    "    topic_vector = [prob for _, prob in topic_distribution]\n",
    "    return topic_vector\n",
    "\n",
    "# Apply to all documents\n",
    "content_topic_vectors = [get_topic_vector(doc) for doc in corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4299d-9c15-4c80-bc67-7be1fd08bb9c",
   "metadata": {},
   "source": [
    "\n",
    "Evaluation\n",
    "\n",
    "Evaluating an LDA (Latent Dirichlet Allocation) model is an important step in topic modeling to determine the effectiveness of the model in discovering underlying topics in a corpus of text documents. There are several methods for evaluating an LDA model, including:\n",
    "\n",
    "    Perplexity: Perplexity is a commonly used metric for evaluating LDA models. It measures how well the model predicts new data. A lower perplexity score indicates better performance. You can calculate perplexity using the log_perplexity() method of the LDA model in gensim.\n",
    "\n",
    "    Coherence: Coherence measures the degree of semantic similarity between the top N words in each topic. Higher coherence scores indicate better topic quality. You can calculate coherence using the CoherenceModel class in gensim.\n",
    "\n",
    "    Visualization: Visualization can be used to visually inspect the quality of the topics generated by the LDA model. You can use the pyLDAvis package in Python to create interactive visualizations of the topics.\n",
    "\n",
    "    Human evaluation: Finally, you can also perform human evaluation by having human annotators review the topics generated by the LDA model and provide feedback on the quality and interpretability of the topics.\n",
    "\n",
    "In summary, evaluating an LDA model involves using one or more of the above methods to measure the quality of the topics generated by the model. By evaluating the model, you can make adjustments to the parameters and improve the quality of the topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc72493f-c009-43c6-a916-cacdceeadb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.338984215079103"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3db778c-fef1-48b0-aa94-6e4e7ea6a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.3823179442671656\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_content,\n",
    "                                     dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score:', coherence_lda) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1f35f-f400-4c20-9ca0-6d3d175e0208",
   "metadata": {},
   "source": [
    "\n",
    "Hyperparameter TuningÂ¶\n",
    "\n",
    "First, letâ€™s differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "    Number of Topics (K)\n",
    "    Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "    Dirichlet hyperparameter beta: Word-Topic Density\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1efccdfd-27c3-4ce2-a84f-1667ac982c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    lda_model = models.LdaModel(corpus=corpus, num_topics=k,\n",
    "                                id2word=dictionary, passes=10,\n",
    "                                alpha=a, eta=b, chunksize=100,\n",
    "                                per_word_topics=True, random_state=42)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, \n",
    "                                         texts=preprocessed_content,\n",
    "                                         dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbda7e9-11fd-4eb0-bdc3-d15b74fa2b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45e4d11ccef42828adbdc7231d8651b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from gensim.utils import ClippedCorpus\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 10\n",
    "max_topics = 30\n",
    "step_size = 5\n",
    "topics_range = range(min_topics, max_topics+1, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "# results placeholder\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# tqdm progress bar\n",
    "pbar = tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)),\n",
    "                 file=sys.stdout, colour='green')\n",
    "\n",
    "### takes ~ 37 minutes\n",
    "for i in range(len(corpus_sets)):\n",
    "    for k in topics_range:    # iterate through validation corpuses\n",
    "        for a in alpha:       # iterate through alpha values\n",
    "            for b in beta:    # iterare through beta values\n",
    "                # get the coherence score for the given parameters\n",
    "                cv = compute_coherence_values(corpus=corpus_sets[i],\n",
    "                                              dictionary=dictionary,\n",
    "                                              k=k, a=a, b=b)\n",
    "                # Save the model results\n",
    "                model_results['Validation_Set'].append(corpus_title[i])\n",
    "                model_results['Topics'].append(k)\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Beta'].append(b)\n",
    "                model_results['Coherence'].append(cv)\n",
    "                \n",
    "                # update tqdm progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.refresh()\n",
    "pbar.close()\n",
    "\n",
    "# save the results to a dataframe\n",
    "res_df = pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a007d5-0631-466d-aaea-a8785cdd92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.sort_values(by='Coherence', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5631c2-5f4e-47c5-830a-f263d9fcac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "res_df.Coherence.plot(kind='line', title='Coherence Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea76ca6-7c6a-4b4f-b3a7-437eb160194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.Coherence.plot(kind='hist', title='Coherence Score Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a38f33-dcfe-4593-992e-4d230fd6f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.plot.scatter(x='Topics', y='Coherence', title='Topics vs Coherence');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada0591-4e12-4deb-8f98-0da7dd00c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.groupby('Topics')['Coherence'].mean().plot(title='Topics Coherence Mean');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
