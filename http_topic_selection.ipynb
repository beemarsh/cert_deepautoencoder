{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9083e7a9-35f2-4ed9-b2fc-ac2e3ba22382",
   "metadata": {},
   "source": [
    "# Topic Number selection for HTTP Content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58fb007a-4300-4bcf-8f23-7287f56f0f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 22:17:39.902189: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os,datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "\n",
    "import multiprocessing as mp\n",
    "from thinc.api import set_gpu_allocator, require_gpu\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import cycle\n",
    "import cupy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bf5321-f804-4ee7-9f93-7b07e4d3e21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.0)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /homes/01/bxbhusal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /homes/01/bxbhusal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /homes/01/bxbhusal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download some useful packages\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "# Download NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e314308-b30e-42d4-be78-2670b9296e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare some userful variables\n",
    "num_visible_rows=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944b330-4ac4-410a-8b38-091dc98dd531",
   "metadata": {},
   "source": [
    "## Take HTTP data as input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb06a70-fa90-4517-b258-1107da8ef28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{Q5R1-T3EF87UE-2395RWZS}</td>\n",
       "      <td>2010-01-02 07:00:13</td>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{X9O1-O0XW52VO-5806RPHG}</td>\n",
       "      <td>2010-01-02 07:03:46</td>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{G5S8-U5OG04TE-5299CCTU}</td>\n",
       "      <td>2010-01-02 07:05:26</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{L0R4-A9DH29VP-4553AUWM}</td>\n",
       "      <td>2010-01-02 07:05:52</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{U7D0-K8FF04MI-4691ZHYP}</td>\n",
       "      <td>2010-01-02 07:05:55</td>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://skype.com/William_D_Boyce/lsa/onpxcnpxp...</td>\n",
       "      <td>feed commonly reef frogs years replaced walter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>{P4C0-H4AC44QZ-5898ATQE}</td>\n",
       "      <td>2010-01-05 08:24:12</td>\n",
       "      <td>BBS0039</td>\n",
       "      <td>PC-9436</td>\n",
       "      <td>http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...</td>\n",
       "      <td>an unknown afternoon dietary state law nationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>{U9Y2-H0WW90NF-1184ROHW}</td>\n",
       "      <td>2010-01-05 08:24:12</td>\n",
       "      <td>KAL0395</td>\n",
       "      <td>PC-0004</td>\n",
       "      <td>http://tigerdirect.com/European_Commission/bar...</td>\n",
       "      <td>begin top we band themselves harshly or fourth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>{Y2Y4-T1KW57MX-2017DWRE}</td>\n",
       "      <td>2010-01-05 08:24:12</td>\n",
       "      <td>LDB0090</td>\n",
       "      <td>PC-6824</td>\n",
       "      <td>http://microsoft.com/Meteorological_history_of...</td>\n",
       "      <td>until acquire flared get secondary minas sea u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>{X2D5-E5UH86FZ-2678DVEF}</td>\n",
       "      <td>2010-01-05 08:24:12</td>\n",
       "      <td>WTF0387</td>\n",
       "      <td>PC-6159</td>\n",
       "      <td>http://sidereel.com/Miniopterus_griveaudi/griv...</td>\n",
       "      <td>form air rely conference quickly set expected ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>{G9F6-M8EE60PG-3261CVQB}</td>\n",
       "      <td>2010-01-05 08:24:13</td>\n",
       "      <td>BKW0374</td>\n",
       "      <td>PC-5186</td>\n",
       "      <td>http://eonline.com/Gregory_of_Nazianzus/sasima...</td>\n",
       "      <td>speak convened end needed down next us church ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id                date     user       pc  \\\n",
       "0      {Q5R1-T3EF87UE-2395RWZS} 2010-01-02 07:00:13  NGF0157  PC-6056   \n",
       "1      {X9O1-O0XW52VO-5806RPHG} 2010-01-02 07:03:46  NGF0157  PC-6056   \n",
       "2      {G5S8-U5OG04TE-5299CCTU} 2010-01-02 07:05:26  IRM0931  PC-7188   \n",
       "3      {L0R4-A9DH29VP-4553AUWM} 2010-01-02 07:05:52  IRM0931  PC-7188   \n",
       "4      {U7D0-K8FF04MI-4691ZHYP} 2010-01-02 07:05:55  IRM0931  PC-7188   \n",
       "...                         ...                 ...      ...      ...   \n",
       "99995  {P4C0-H4AC44QZ-5898ATQE} 2010-01-05 08:24:12  BBS0039  PC-9436   \n",
       "99996  {U9Y2-H0WW90NF-1184ROHW} 2010-01-05 08:24:12  KAL0395  PC-0004   \n",
       "99997  {Y2Y4-T1KW57MX-2017DWRE} 2010-01-05 08:24:12  LDB0090  PC-6824   \n",
       "99998  {X2D5-E5UH86FZ-2678DVEF} 2010-01-05 08:24:12  WTF0387  PC-6159   \n",
       "99999  {G9F6-M8EE60PG-3261CVQB} 2010-01-05 08:24:13  BKW0374  PC-5186   \n",
       "\n",
       "                                                     url  \\\n",
       "0      http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "1      http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "2      http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "3      http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "4      http://skype.com/William_D_Boyce/lsa/onpxcnpxp...   \n",
       "...                                                  ...   \n",
       "99995  http://stubhub.com/Hoover_Dam/ickes/zbgbeplpyr...   \n",
       "99996  http://tigerdirect.com/European_Commission/bar...   \n",
       "99997  http://microsoft.com/Meteorological_history_of...   \n",
       "99998  http://sidereel.com/Miniopterus_griveaudi/griv...   \n",
       "99999  http://eonline.com/Gregory_of_Nazianzus/sasima...   \n",
       "\n",
       "                                                 content  \n",
       "0      festival off northwards than congestion partne...  \n",
       "1      long away reorganized baldwin seth business 18...  \n",
       "2      among german schwein experimental becomes prev...  \n",
       "3      kate criteria j 2008 highest 12 include books ...  \n",
       "4      feed commonly reef frogs years replaced walter...  \n",
       "...                                                  ...  \n",
       "99995  an unknown afternoon dietary state law nationa...  \n",
       "99996  begin top we band themselves harshly or fourth...  \n",
       "99997  until acquire flared get secondary minas sea u...  \n",
       "99998  form air rely conference quickly set expected ...  \n",
       "99999  speak convened end needed down next us church ...  \n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./r4.2/http.csv\", header=0, nrows=100000, skiprows=(1,500000))\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0440f9-d633-48be-90b9-8ba2e7cd390b",
   "metadata": {},
   "source": [
    "## Extract features from Date!\n",
    "The user logs are at different times. I divided the time into **4** different time frames.\n",
    "- 0 = 12AM - 6AM\n",
    "- 1 = 6AM - 12PM\n",
    "- 2 = 12PM - 6PM\n",
    "- 3 = 6PM - 12AM\n",
    "Therefore, a new feature **time_frame** is made. Date is decomposed into 3 other numerical features: `day`,`month`, and `year`. Finally, date feature is dropped.\n",
    "\n",
    "After dividng them into 4 different time frames in order to apply one-hot encoding.\n",
    "\n",
    "**Why One-Hot?**\n",
    "  - **No Ordinality:** Each time frame is represented independently without implying any order.\n",
    "  - **Clarity:** Clearly distinguishes between different time frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d1758c-b0cb-4bd7-b1b0-5cae3f03e3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>time_frame_1</th>\n",
       "      <th>time_frame_2</th>\n",
       "      <th>time_frame_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NGF0157</td>\n",
       "      <td>PC-6056</td>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IRM0931</td>\n",
       "      <td>PC-7188</td>\n",
       "      <td>http://skype.com/William_D_Boyce/lsa/onpxcnpxp...</td>\n",
       "      <td>feed commonly reef frogs years replaced walter...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user       pc                                                url  \\\n",
       "0  NGF0157  PC-6056  http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "1  NGF0157  PC-6056  http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "2  IRM0931  PC-7188  http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "3  IRM0931  PC-7188  http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "4  IRM0931  PC-7188  http://skype.com/William_D_Boyce/lsa/onpxcnpxp...   \n",
       "\n",
       "                                             content  day  month  year  \\\n",
       "0  festival off northwards than congestion partne...    2      1  2010   \n",
       "1  long away reorganized baldwin seth business 18...    2      1  2010   \n",
       "2  among german schwein experimental becomes prev...    2      1  2010   \n",
       "3  kate criteria j 2008 highest 12 include books ...    2      1  2010   \n",
       "4  feed commonly reef frogs years replaced walter...    2      1  2010   \n",
       "\n",
       "   time_frame_1  time_frame_2  time_frame_3  \n",
       "0             1             0             0  \n",
       "1             1             0             0  \n",
       "2             1             0             0  \n",
       "3             1             0             0  \n",
       "4             1             0             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function to categorize time frames\n",
    "def categorize_time_frame(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 0\n",
    "    elif 6 <= hour < 12:\n",
    "        return 1\n",
    "    elif 12 <= hour < 18:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Apply the function to create the 'time_frame' column\n",
    "new_df = df.copy()\n",
    "new_df['time_frame'] = df['date'].dt.hour.apply(categorize_time_frame)\n",
    "new_df['day'] = df['date'].dt.day\n",
    "new_df['month'] = df['date'].dt.month\n",
    "new_df['year'] = df['date'].dt.year\n",
    "\n",
    "new_df=new_df.drop(columns=\"date\")\n",
    "new_df=new_df.drop(columns=\"id\")\n",
    "\n",
    "# Applying one-hot encoding\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "time_encoded = ohe.fit_transform(new_df[['time_frame']])\n",
    "\n",
    "# Create DataFrame with One-Hot Encoded Columns\n",
    "time_encoded_df = pd.DataFrame(time_encoded.astype(int), columns=ohe.get_feature_names_out(['time_frame']))\n",
    "\n",
    "# Concatenate with Original DataFrame\n",
    "new_df = pd.concat([new_df, time_encoded_df], axis=1).drop('time_frame', axis=1)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae897ccb-ace9-4038-acdd-ec736429d1ee",
   "metadata": {},
   "source": [
    "# Encoding Categorical Features: Binary Encoding vs. Label Encoding\n",
    "In this part, I am encoding USER and PC column into numerical features.\n",
    "\n",
    "## **Why Choose Binary Encoding Over Label Encoding?**\n",
    "\n",
    "- **Avoids Artificial Ordinality:** Unlike Label Encoding, Binary Encoding doesn't introduce an arbitrary numerical order among categories, preventing the model from misinterpreting relationships.\n",
    "- **Dimensionality Efficiency:** Binary Encoding transforms categories into a compact binary format, significantly reducing the number of features compared to One-Hot Encoding while preserving uniqueness.\n",
    "- **Scalability:** Handles high-cardinality features (e.g., 1,000 unique users) efficiently without creating thousands of columns.\n",
    "\n",
    "## **How They Work**\n",
    "\n",
    "### **Label Encoding**\n",
    "\n",
    "Assigns each unique category a distinct integer.\n",
    "\n",
    "| Category | Label Encoded |\n",
    "|----------|---------------|\n",
    "| User1    | 0             |\n",
    "| User2    | 1             |\n",
    "| User3    | 2             |\n",
    "| ...      | ...           |\n",
    "| User1000 | 999           |\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to implement.\n",
    "- Low dimensionality (single column).\n",
    "\n",
    "**Cons:**\n",
    "- Introduces artificial ordinality.\n",
    "- May mislead models to infer unintended relationships.\n",
    "\n",
    "### **Binary Encoding**\n",
    "\n",
    "Converts each category to its binary representation spread across multiple binary columns.\n",
    "\n",
    "| Category | Binary Encoded (10 bits) |\n",
    "|----------|--------------------------|\n",
    "| User1    | 0000000000               |\n",
    "| User2    | 0000000001               |\n",
    "| User3    | 0000000010               |\n",
    "| ...      | ...                      |\n",
    "| User1000 | 1111100111               |\n",
    "\n",
    "**Pros:**\n",
    "- Avoids artificial ordinality.\n",
    "- More compact than One-Hot Encoding (~10 columns for 1,000 categories).\n",
    "- Preserves uniqueness of categories.\n",
    "\n",
    "**Cons:**\n",
    "- Slightly more complex implementation.\n",
    "- Binary features are less interpretable individually.\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "**Binary Encoding** offers a balanced approach for high-cardinality categorical features by maintaining uniqueness without inflating dimensionality or introducing artificial order, making it a superior choice over **Label Encoding** for scenarios like representing 1,000 unique users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf925d8-f7f4-4d5d-93a6-8cbc6ea920dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Encoder for user\n",
    "user_binary_encoder = ce.BinaryEncoder(cols=['user'])\n",
    "\n",
    "# Fit and transform the 'user' column\n",
    "new_df = user_binary_encoder.fit_transform(new_df)\n",
    "\n",
    "#At last, save the encoder for further use.\n",
    "pkl_user_encoder_output = open(\"user_encoder.pkl\",'wb')\n",
    "pickle.dump(user_binary_encoder, pkl_user_encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930e3563-230e-40de-a9cc-ebf6c8bba4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_0</th>\n",
       "      <th>user_1</th>\n",
       "      <th>user_2</th>\n",
       "      <th>user_3</th>\n",
       "      <th>user_4</th>\n",
       "      <th>user_5</th>\n",
       "      <th>user_6</th>\n",
       "      <th>user_7</th>\n",
       "      <th>user_8</th>\n",
       "      <th>user_9</th>\n",
       "      <th>...</th>\n",
       "      <th>pc_8</th>\n",
       "      <th>pc_9</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>time_frame_1</th>\n",
       "      <th>time_frame_2</th>\n",
       "      <th>time_frame_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>http://urbanspoon.com/Plunketts_Creek_Loyalsoc...</td>\n",
       "      <td>festival off northwards than congestion partne...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...</td>\n",
       "      <td>long away reorganized baldwin seth business 18...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://groupon.com/Leonhard_Euler/leonhard/tne...</td>\n",
       "      <td>among german schwein experimental becomes prev...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://flickr.com/Inauguration_of_Barack_Obama...</td>\n",
       "      <td>kate criteria j 2008 highest 12 include books ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>http://skype.com/William_D_Boyce/lsa/onpxcnpxp...</td>\n",
       "      <td>feed commonly reef frogs years replaced walter...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_0  user_1  user_2  user_3  user_4  user_5  user_6  user_7  user_8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       1   \n",
       "3       0       0       0       0       0       0       0       0       1   \n",
       "4       0       0       0       0       0       0       0       0       1   \n",
       "\n",
       "   user_9  ...  pc_8  pc_9                                                url  \\\n",
       "0       1  ...     0     1  http://urbanspoon.com/Plunketts_Creek_Loyalsoc...   \n",
       "1       1  ...     0     1  http://aa.com/Rhodocene/rhodocenium/fhaavatqrf...   \n",
       "2       0  ...     1     0  http://groupon.com/Leonhard_Euler/leonhard/tne...   \n",
       "3       0  ...     1     0  http://flickr.com/Inauguration_of_Barack_Obama...   \n",
       "4       0  ...     1     0  http://skype.com/William_D_Boyce/lsa/onpxcnpxp...   \n",
       "\n",
       "                                             content  day  month  year  \\\n",
       "0  festival off northwards than congestion partne...    2      1  2010   \n",
       "1  long away reorganized baldwin seth business 18...    2      1  2010   \n",
       "2  among german schwein experimental becomes prev...    2      1  2010   \n",
       "3  kate criteria j 2008 highest 12 include books ...    2      1  2010   \n",
       "4  feed commonly reef frogs years replaced walter...    2      1  2010   \n",
       "\n",
       "   time_frame_1  time_frame_2  time_frame_3  \n",
       "0             1             0             0  \n",
       "1             1             0             0  \n",
       "2             1             0             0  \n",
       "3             1             0             0  \n",
       "4             1             0             0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary Encoder for PC\n",
    "pc_binary_encoder = ce.BinaryEncoder(cols=['pc'])\n",
    "\n",
    "# Fit and transform the 'pc' column\n",
    "new_df = pc_binary_encoder.fit_transform(new_df)\n",
    "\n",
    "#At last, save the encoder for further use.\n",
    "pkl_pc_encoder_output = open(\"pc_encoder.pkl\",'wb')\n",
    "pickle.dump(pc_binary_encoder, pkl_pc_encoder_output)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36172f3e-6787-4bc2-b41d-9b22734d68f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf6176b-fe02-41c5-a57c-1b1d689a4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize stopwords, lemmatizer, and punctuation\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# punctuations = string.punctuation\n",
    "\n",
    "# spacy.require_gpu()\n",
    "\n",
    "# # Load spaCy's English model\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # Remove punctuation\n",
    "#     text = text.translate(str.maketrans('', '', punctuations))\n",
    "    \n",
    "#     # Tokenize using spaCy for efficient lemmatization\n",
    "#     doc = nlp(text)\n",
    "    \n",
    "#     # Lemmatize and remove stopwords and non-alphabetic tokens\n",
    "#     tokens = [token.lemma_ for token in doc if token.lemma_.isalpha() and token.lemma_ not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# # Apply preprocessing to the 'content' column\n",
    "# new_df['tokens'] = new_df['content'].apply(preprocess_text)\n",
    "\n",
    "# # Display the tokens\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a297c08-a00d-4a6b-a785-e2c21315778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# def preprocess_texts(texts, gpu_id):\n",
    "#     \"\"\"\n",
    "#     Preprocess a list of texts using spaCy on a specific GPU.\n",
    "    \n",
    "#     Args:\n",
    "#         texts (list): List of text strings to preprocess.\n",
    "#         gpu_id (int): GPU ID to assign to this process.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of token lists.\n",
    "#     \"\"\"\n",
    "#     # Assign the process to a specific GPU\n",
    "#     spacy.require_gpu(gpu_id)\n",
    "    \n",
    "    \n",
    "#     # Load spaCy model with GPU\n",
    "#     nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    \n",
    "#     # Initialize tools\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     punctuations = string.punctuation\n",
    "    \n",
    "#     processed_texts = []\n",
    "#     for doc in nlp.pipe(texts, batch_size=1000):\n",
    "#         tokens = [\n",
    "#             token.lemma_ for token in doc \n",
    "#             if token.lemma_.isalpha() and token.lemma_ not in stop_words\n",
    "#         ]\n",
    "#         processed_texts.append(tokens)\n",
    "    \n",
    "#     return processed_texts\n",
    "\n",
    "\n",
    "# def split_data(texts, num_chunks):\n",
    "#     \"\"\"\n",
    "#     Split the list of texts into specified number of chunks.\n",
    "    \n",
    "#     Args:\n",
    "#         texts (list): List of text strings.\n",
    "#         num_chunks (int): Number of chunks to split into.\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List containing chunks of texts.\n",
    "#     \"\"\"\n",
    "#     chunk_size = len(texts) // num_chunks\n",
    "#     return [texts[i*chunk_size : (i+1)*chunk_size] for i in range(num_chunks-1)] + [texts[(num_chunks-1)*chunk_size:]]\n",
    "\n",
    "# # Convert the text into an array of texts\n",
    "# texts = new_df['content'].tolist()\n",
    "\n",
    "# chunks = split_data(texts, num_gpus)\n",
    "\n",
    "# pool = mp.Pool(processes=num_gpus)\n",
    "\n",
    "# # Prepare arguments for each process\n",
    "# args = [(chunk, gpu_id) for gpu_id, chunk in enumerate(chunks)]\n",
    "\n",
    "# # Execute preprocessing in parallel\n",
    "# results = pool.starmap(preprocess_texts, args)\n",
    "\n",
    "# # Close the pool\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb2367c-81b9-4768-acf8-c39310634b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set multiprocessing start method\n",
    "# mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3298d69f-3759-4c0b-ba95-afdbfb11dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Processing on GPU 0Processing on GPU 1\n",
      "\n",
      "Processing on GPU 0Processing on GPU 1\n",
      "\n"
     ]
    },
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorInitializationError: initialization error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n  File \"/homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/homes/01/bxbhusal/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/tmp/ipykernel_151269/816234820.py\", line 36, in process_chunk\n    with cupy.cuda.Device(rank):\n  File \"cupy/cuda/device.pyx\", line 173, in cupy.cuda.device.Device.__enter__\n  File \"cupy_backends/cuda/api/runtime.pyx\", line 202, in cupy_backends.cuda.api.runtime.getDevice\n  File \"cupy_backends/cuda/api/runtime.pyx\", line 146, in cupy_backends.cuda.api.runtime.check_status\ncupy_backends.cuda.api.runtime.CUDARuntimeError: cudaErrorInitializationError: initialization error\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m texts \u001b[38;5;241m=\u001b[39m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Preprocess the texts using both GPUs\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 73\u001b[0m, in \u001b[0;36mpreprocess_parallel\u001b[0;34m(texts, chunksize)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tasks))\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Use joblib's Parallel to process chunks in parallel\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmultiprocessing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Flatten the list of results\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flatten(results)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorInitializationError: initialization error"
     ]
    }
   ],
   "source": [
    "def chunker(iterable, total_length, chunksize):\n",
    "    \"\"\"Yield successive chunks from iterable.\"\"\"\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list of lists into a single list.\"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def process_entity(doc):\n",
    "    \"\"\"Process a spaCy document to extract relevant tokens.\"\"\"\n",
    "    super_word_ls = []\n",
    "    for s in doc.sents:\n",
    "        word_ls = []\n",
    "        for t in s:\n",
    "            if not t.ent_type_:\n",
    "                if t.text.strip() != \"\":\n",
    "                    word_ls.append(t.text)\n",
    "            else:\n",
    "                word_ls.append(t.ent_type_)\n",
    "        if len(word_ls) > 0:\n",
    "            super_word_ls.append(\" \".join(word_ls))\n",
    "    return \" \".join(super_word_ls)\n",
    "\n",
    "def process_chunk(texts, rank):\n",
    "    \"\"\"\n",
    "    Process a chunk of texts on a specific GPU.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of text strings to process.\n",
    "        rank (int): GPU ID to assign to this process.\n",
    "\n",
    "    Returns:\n",
    "        list: List of processed text strings.\n",
    "    \"\"\"\n",
    "    print(f\"Processing on GPU {rank}\")\n",
    "    with cupy.cuda.Device(rank):\n",
    "        set_gpu_allocator(\"pytorch\")\n",
    "        require_gpu(rank)\n",
    "        # Load the transformer-based spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_trf\")\n",
    "        preproc_pipe = []\n",
    "        for doc in nlp.pipe(texts, batch_size=20):\n",
    "            preproc_pipe.append(process_entity(doc))\n",
    "        return preproc_pipe\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    \"\"\"\n",
    "    Parallelize text preprocessing across multiple GPUs.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of text strings to preprocess.\n",
    "        chunksize (int): Number of texts per chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: List of all processed text strings.\n",
    "    \"\"\"\n",
    "    num_gpus = cupy.cuda.runtime.getDeviceCount()\n",
    "    gpus = list(range(num_gpus))\n",
    "    rank_cycle = cycle(gpus)\n",
    "\n",
    "    \n",
    "    # Split texts into chunks\n",
    "    chunks = list(chunker(texts, len(texts), chunksize))\n",
    "    \n",
    "    # Assign each chunk to a GPU in a round-robin fashion\n",
    "    tasks = [(chunk, next(rank_cycle)) for chunk in chunks]\n",
    "\n",
    "    print(len(tasks))\n",
    "    \n",
    "    # Use joblib's Parallel to process chunks in parallel\n",
    "    results = Parallel(n_jobs=num_gpus, backend='multiprocessing')(\n",
    "        delayed(process_chunk)(chunk, rank) for chunk, rank in tasks\n",
    "    )\n",
    "    \n",
    "    # Flatten the list of results\n",
    "    return flatten(results)\n",
    "\n",
    "\n",
    "# Convert the text into an array of texts\n",
    "texts = new_df['content'].tolist()\n",
    "\n",
    "# Preprocess the texts using both GPUs\n",
    "processed_texts = preprocess_parallel(texts, chunksize=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c596292-69df-4c1e-8e10-b17e76ca9977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.']\n",
      "['This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.']\n",
      "['This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.']\n",
      "['This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.', 'This is a basic sentence. This is another one.']\n"
     ]
    }
   ],
   "source": [
    "# A different method for flattening a list\n",
    "def flatten2d(list2d):\n",
    "    from functools import reduce\n",
    "    from operator import iconcat\n",
    "    return reduce(iconcat, list2d, [])\n",
    "\n",
    "def chunker(iterator, length, chunksize):\n",
    "    return (iterator[pos: pos + chunksize] for pos in range(0, length, chunksize))\n",
    "\n",
    "def process_entity(doc):\n",
    "    # I need lists of sentences for my use case,  but you could do other processing\n",
    "    return [s.text for s in doc.sents]\n",
    "\n",
    "def process_chunk(docs, rank):\n",
    "    with cupy.cuda.Device(rank):\n",
    "        set_gpu_allocator('pytorch')\n",
    "        require_gpu(rank)\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "        nlp.add_pipe('sentencizer')\n",
    "        preprocess_pipe = []\n",
    "        for doc in nlp.pipe(docs, batch_size=20):\n",
    "            preprocess_pipe.append(process_entity(doc))\n",
    "        rank += 1\n",
    "        return preprocess_pipe\n",
    "\n",
    "def process_parallel(docs, jobs=2, chunksize=50):\n",
    "    executor = Parallel(n_jobs=jobs, prefer='threads')\n",
    "    \n",
    "    do = delayed(process_chunk)\n",
    "    \n",
    "    tasks = []\n",
    "    gpus = list(range(0, cupy.cuda.runtime.getDeviceCount()))\n",
    "    \n",
    "    rank = 0\n",
    "    \n",
    "    for chunk in chunker(docs, len(docs), chunksize):\n",
    "        tasks.append(do(chunk, rank))\n",
    "        rank = (rank + 1) % len(gpus)\n",
    "\n",
    "    \n",
    "    result = executor(tasks)\n",
    "    return flatten2d(result)\n",
    "\n",
    "\n",
    "preprocessed = process_parallel(docs = [\"This is a basic sentence. This is another one.\"]*100, jobs=4, chunksize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a81f29cb-cb8f-44d2-9c3c-4f4c5fed9b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.'],\n",
       " ['This is a basic sentence.', 'This is another one.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e373b17-4e2d-433e-8c02-d0a2e697e42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
